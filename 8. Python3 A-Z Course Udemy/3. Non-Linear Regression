
1) Support Vector Regression
2) Decision Tree Regression
3) Random Forest Regression
4) 

Note- Visualization is applicable if there are two indepdent variables only

#############################      1.  Support Vector Regression            ######################################

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv("Position_Salaries.csv")
X = df.iloc[:,1:2].values
y = df.iloc[:, 2].values
y = y.reshape(-1,1)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X = sc_X.fit_transform(X)
y = sc_y.fit_transform(y)




from sklearn.svm import SVR
regressor = SVR(kernel='rbf')
regressor.fit(X, y)

# Prediction a new result 6.5 yrs
y_pred = regressor.predict(6.5)

# Scaled y for prediction
# Here we want the prediction where y=6.5. Before prediction, we wanna scale it using X.transform. Transform
# function takes only np arrays, hence we have made a 1*1 (we use double bracket [[]] for 1*1 matrix) array of np
# This functiin wuld give tranformed (scaled) prediction of Y, hence we unscaled it using inverse transform 
 ## y_pred1 = sc_y.inverse_transform(regressor.predict(sc_X.transform(np.array([[6.5]]))))

# Visualizing the SVR Results
plt.scatter(X, y, color='red')
plt.plot(X, regressor.predict(X), color='blue')
plt.title('Truth or Bluff (SVR)')
plt.xlabel('Position Level')
plt.ylabel('Salary')
plt.show()



# Visualising the SVR results (for higher resolution and smoother curve)
X_grid = np.arange(min(X), max(X), 0.01) # choice of 0.01 instead of 0.1 step because the data is feature scaled
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (SVR)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()




#############################      2.  DECISION TREE REGRESSION            ######################################


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv('Position_Salaries.csv')
X = df.iloc[:,1:2].values
y = df.iloc[:,2].values
y = y.reshape(-1,1)



from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor(random_state=0)
regressor.fit(X, y)

y_pred = regressor.predict(6.5)


plt.scatter(X, y, color='red')
plt.plot(X, regressor.predict(X), color='blue')
plt.title('Truth or Bluff (Decision Tree Regression)')
plt.xlabel('Position Level')
plt.ylabel('Salary')
plt.show()


# Visualising the Decision Tree Regression results (higher resolution)
X_grid = np.arange(min(X), max(X), 0.01)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (Decision Tree Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()




#############################      3.  Random Forest Regression            ######################################

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv('Position_Salaries.csv')
X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values
y = y.reshape(-1,1)


# If take more trees (n_estimator), prediction would be even better
from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators=10, random_state=0)
regressor.fit(X, y)
# regressor = RandomForestRegressor(n_estimators=100, random_state=0)

y_pred = regressor.predict(6.5)

# Visualising the SVR results (for higher resolution and smoother curve)
X_grid = np.arange(min(X), max(X), 0.01) # choice of 0.01 instead of 0.1 step because the data is feature scaled
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (Random Forest Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()







