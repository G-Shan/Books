


1) Logistic Regression
2) K-Nearest Neighbor
3) Support Vector Classfier
4) Support Vector Machine
5) Naive Bayes Classifier
6) Decision Tree
7) Random Forest

Note- Visualization is applicable if there are two indepdent variables only

##################################         1. Logistic Regression           ################################

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv('Social_Network_Ads.csv')
X = df.iloc[:,[2,3]].values
y = df.iloc[:,4].values
#y = y.reshape(-1,1)
#X[:,0:2] = X[:,0:2].astype(float) # X is into integer, it would give warning when spilling into test train hence conversion


# Splitting the dataset into training and test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)


# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

# Model training and prediction on test set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)


# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)


# Visulazation
from mlxtend.plotting import plot_decision_regions
plot_decision_regions(X_test, y_test, clf=classifier, legend=2, colors=('red,green'))
plt.title('Logistic Regression (Test Set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.show()


# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()


# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test



##################################         2. K-Nearest Neighbor           ################################

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
df = pd.read_csv('Social_Network_Ads.csv')
X = df.iloc[:,[2,3]].values
y = df.iloc[:,4].values

# Splitting the dataset into training and test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p=2)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visulazation on train set
from mlxtend.plotting import plot_decision_regions
plot_decision_regions(X_train, y_train, clf=classifier, legend=2, colors=('red,green'))
plt.title('K Nearest Neighbor (Test Set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.show()


# Visulazation on test set
plot_decision_regions(X_test, y_test, clf=classifier, legend=2, colors=('red,green'))



##################################         3. Support Vector Classifier           ################################

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv('Social_Network_Ads.csv')
X = df.iloc[:,[2,3]].values
y = df.iloc[:,4].values

# Splitting the dataset into training and test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)


from sklearn.svm import SVC
classifier = SVC(kernel='linear', random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)


# Visulazation on train and test set



##################################         4. Support Vector Machine           ################################

import libraries
import dataset Social_Network_Ads.csv and create X and y
Splitting the dataset into training and test set
Feature Scaling

from sklearn.svm import SVC
classifier = SVC(kernel='rbf', random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visulazation on train and test set




##################################         5. Naive Bayes Classifier           ################################

Bayes Classifier:- Itâ€™s an classifier that minimizes the error in probabilistic manner. When we calculate a binary classifier, we
just sum up all the mismatches wherein each mismatch is considered as 1. If it bayes optimal classifier, the errors are weighted using
the joint probability between the input & output sets.

Bayes Theorem:- P(A/B) = [P(B/A) * P(A)]/P(B)

E.g. Suppose there are two machines that produce wrenches. 1st machine produced wrenches 30 piece per hour while 2nd machine
produced 20 per hour. Out of all the wrenches produced from two machines, 1% of the wrenches are defective. Out of all the defective
wrenches, 50% are produced by machine 1 while rest are produced by Machine 2. What is the probability that a wrench produced by mach2
is defective?

Ans- P(Mach1) = 30/50 = 0.6,       P(Mach2) = 20/50 = 0.4,      P(Defect) = 1%
         P(Mach1 | Defect) = 50%,       P(Mach2 | Defect) = 50%
P(Defect | Mach2) = [P(Mach2 | Defect) * P(Defect)]/P(Mach2) = (0.5 * 0.01)/0.4 = 0.0125



##########

import libraries
import dataset Social_Network_Ads.csv and create X and y
Splitting the dataset into training and test set
Feature Scaling

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visulazation on train and test set


##################################         6. Decision Tree           ################################

import libraries
import dataset Social_Network_Ads.csv and create X and y
Splitting the dataset into training and test set
Feature Scaling

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visulazation on train and test set




##################################         7. Random Forest           ################################

import libraries
import dataset Social_Network_Ads.csv and create X and y
Splitting the dataset into training and test set
Feature Scaling

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

